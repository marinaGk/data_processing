{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis for HRV and MET received through device\n",
    "\n",
    "Using MAXIM maxref103 to receive data through bluetooth. \n",
    "\n",
    "Data received come in the format of raw PPG with peak detection and accelerometer with sampling of around 25 samples a second. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import csv \n",
    "\n",
    "with open('processed_data.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['index', 'duration', 'RMSSD', 'pNN50', 'HF', 'LF', 'HF/LF', 'MET', 'temperature', 'thermal_comfort']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames= fieldnames)\n",
    "    writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, HRV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data_files/002.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get magnitude of PPG measurement\n",
    "amp = df['green']\n",
    "amp_array = amp.to_numpy()\n",
    "\n",
    "samples = df['sample_count']\n",
    "rr = df['rr']\n",
    "timestamp = df['sample_time']\n",
    "\n",
    "#normalize\n",
    "max = np.max(amp_array)\n",
    "min = np.min(amp_array)\n",
    "\n",
    "amp_array_norm = (amp_array - min) / (max - min)\n",
    "\n",
    "#get synthetic acceleration \n",
    "acx = df['acceleration_x']\n",
    "acy = df['acceleration_y']\n",
    "acz = df['acceleration_z']\n",
    "\n",
    "syn_ac = np.square(acx) + np.square(acy) + np.square(acz)\n",
    "syn_ac = np.power(syn_ac, 1/2)\n",
    "\n",
    "#get rr and clean rr\n",
    "rr_nz = rr[rr!=0]\n",
    "rr_array = rr_nz.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Denoising by outlier detection and removal. Zero time data have already been removed from RR interval signal (since it is sampled at the same time as raw PPG)\n",
    "\n",
    "Remains to be done: try denoising by acceleration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do IQR on RR signal to remove outliers and clean rr\n",
    "import numpy as np\n",
    "\n",
    "Q1 = np.percentile(rr_array, 25)\n",
    "Q3 = np.percentile(rr_array, 75)\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "#print(IQR)\n",
    "\n",
    "iqr_min = Q1 - 1.5*IQR\n",
    "iqr_max = Q3 + 1.5*IQR\n",
    "\n",
    "rr_array_clean = rr_array[(rr_array <iqr_max) & (rr_array>iqr_min)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting PPG, RR interval and acceleration signals to notice noise trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the PPG, RR, acc plot\n",
    "figure, axis = plt.subplots(3)\n",
    "axis[0].plot(timestamp, amp_array_norm)\n",
    "axis[1].plot(timestamp, rr)\n",
    "axis[2].plot(timestamp, syn_ac)\n",
    "\n",
    "#rr plot seems to follow the direction of PPG measurement as opposed to acceleration which appears to be random here making everything a bit complicated "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIME ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#RMSSD \n",
    "#removing all zero values to get only actual IBIs\n",
    "rr_nz = rr[rr!=0]\n",
    "\n",
    "#RMSSD: root mean square of successive differences\n",
    "#turn this into a function at some point \n",
    "rr_nz_array = rr_nz.to_numpy() #float64 \n",
    "\n",
    "itersize = rr_nz_array.size\n",
    "squares = 0\n",
    "\n",
    "for i in range(itersize-2) : \n",
    "    squares += np.square(rr_nz_array[i+1] - rr_nz_array[i])\n",
    "\n",
    "average = squares / (itersize-1)\n",
    "\n",
    "RMSSD = np.power(average, 1/2)\n",
    "\n",
    "print(RMSSD)\n",
    "\n",
    "#you get RMSSD here but you need to use that later to compare between different samples\n",
    "#turn it into a function to use whenever \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pNN50\n",
    "\n",
    "NN50/total_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN50count(data_array): \n",
    "\n",
    "    NN50count = 0\n",
    "    for i in range(data_array.size - 1):\n",
    "        if np.abs(data_array[i+1]-data_array[i]) > 50: \n",
    "            NN50count += 1     \n",
    "\n",
    "    pNN50 = NN50count/data_array.size\n",
    "    return pNN50\n",
    "\n",
    "def windowing(time_array, start): \n",
    "\n",
    "    timer = start \n",
    "    counter = 0\n",
    "    new_start = 0\n",
    "    while ((time_array.iloc[timer] - time_array.iloc[start]).seconds<120): \n",
    "        counter += 1\n",
    "        timer = start + counter\n",
    "        if (time_array.iloc[timer] - time_array.iloc[start]).seconds == 30: \n",
    "            new_start = timer\n",
    "        if (time_array.iloc[timer] == time_array.iloc[-1]): \n",
    "            break\n",
    "    end = timer\n",
    "\n",
    "    return end, new_start\n",
    "\n",
    "def pNN50(time_array, rr_array):\n",
    "    start = 0\n",
    "    end = 0\n",
    "    percs = []\n",
    "    new_start = 0\n",
    "\n",
    "    while (time_array.iloc[end] != time_array.iloc[-1]):\n",
    "        end, new_start = windowing(time_array, start)\n",
    "        count_array = rr_array[start:end]\n",
    "        pNN50 = NN50count(count_array)\n",
    "        percs.append(pNN50)\n",
    "        start = new_start\n",
    "        \n",
    "\n",
    "    return percs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "#pNN50 is the percentage of counts exceeding/following their next count by 50 ms within two minutes \n",
    "#our timestamp is samples that contain time but don't explicitely express it which means we first need to manipulate those\n",
    "timestamp = df['timestamp']\n",
    "\n",
    "time = pd.to_datetime(timestamp)\n",
    "rr_unclean = rr.to_numpy()\n",
    "\n",
    "#percentages = pNN50(time)\n",
    "percentages = pNN50(time, rr_unclean)\n",
    "\n",
    "pNN50_total = mean(percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPECTRAL ANALYSIS\n",
    "\n",
    "#### FFT with interpolation and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import rfft, rfftfreq\n",
    "from scipy.fft import fft, fftfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets and plots interpolated time data of RR interval\n",
    "fs = 25\n",
    "\n",
    "time_size = rr_array_clean.size\n",
    "times = np.empty(time_size, dtype = int)\n",
    "\n",
    "#clean times \n",
    "for i in range(time_size): \n",
    "    times[i] = i\n",
    "\n",
    "uniform_times = np.arange(0, time_size, 1/fs) #make a uniform time array so as to also uniform the signal and then do FFT\n",
    "from scipy.interpolate import CubicSpline\n",
    "import numpy as np\n",
    "\n",
    "#do cubic spline interpolation which is better suited for HRV\n",
    "cs = CubicSpline(times, rr_array_clean)\n",
    "interpolated_rr = cs(uniform_times)\n",
    "\n",
    "plt.plot(uniform_times, interpolated_rr) #interpolated has negative times which MAKES NO SENSE GENERALLY SPEAKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets and plots FFT of previous data after interpolation\n",
    "\n",
    "from scipy.fft import rfft, rfftfreq\n",
    "n = interpolated_rr.size\n",
    "\n",
    "interpolated_rr = interpolated_rr - np.mean(interpolated_rr) #remove DC \n",
    "\n",
    "rfft = rfft(interpolated_rr)\n",
    "rfftfreq = rfftfreq(n, 1/fs)\n",
    "\n",
    "plt.plot(rfftfreq, np.abs(rfft)) #plots magnitude in frequency of data\n",
    "plt.xlim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets and plots the power of previous spectrum\n",
    "\n",
    "mag = np.abs(rfft)\n",
    "power = mag**2\n",
    "\n",
    "plt.plot(rfftfreq, power)\n",
    "plt.xlim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets the HF, LF, HF/LF parameters to be used in the neural network \n",
    "\n",
    "lf_mask = (rfftfreq >= 0.04) & (rfftfreq <= 0.15)\n",
    "lf_power = np.trapz(power[lf_mask], rfftfreq[lf_mask])\n",
    "\n",
    "hf_mask = (rfftfreq >= 0.15) & (rfftfreq <= 0.4)\n",
    "hf_power = np.trapz(power[hf_mask], rfftfreq[hf_mask])\n",
    "\n",
    "total_power = lf_power + hf_power \n",
    "lf_nu = lf_power/total_power\n",
    "hf_nu = hf_power/total_power\n",
    "ratio = hf_nu/lf_nu\n",
    "\n",
    "print(lf_nu, hf_nu, ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, MET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First method is calculating MET through calories burnt while doing an activity. The files received from the device contain accumulative calories which means I will have to look into the equation a little bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MET = cal / (kg * hours)\n",
    "import pandas as pd \n",
    "\n",
    "kg =  64\n",
    "cal = df['calorie']\n",
    "timestamp = df['sample_time']\n",
    "time_size = timestamp.size\n",
    "\n",
    "#get calories\n",
    "cal_size = cal.size \n",
    "calories = cal[cal_size-1]\n",
    "\n",
    "#get duration of exercise in hours\n",
    "t1 = pd.to_datetime(timestamp[0])\n",
    "t2 = pd.to_datetime(timestamp[time_size-1])\n",
    "duration = (t2-t1).seconds/(60*60)\n",
    "\n",
    "#calculate MET\n",
    "total_MET = calories / (64*duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn this whole thing into one big function and you're done! \n",
    "\n",
    "data = [{\"index\" : None, \"duration\": None, \"RMSSD\": RMSSD, \"pNN50\": None, \"HF\": hf_nu, \"LF\": lf_nu, \"HF/LF\": ratio, \"MET\": total_MET, \"temperature\": None, \"thermal_comfort\": None }]\n",
    "\n",
    "with open('processed_data.csv', 'a') as csvfile:\n",
    "    fieldnames = ['index', 'duration', 'RMSSD', 'pNN50', 'HF', 'LF', 'HF/LF', 'MET', 'temperature', 'thermal_comfort']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames= fieldnames)\n",
    "    writer.writerows(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, neural network\n",
    "\n",
    "I think i will be moving this to its own foldre so i can produce a file here with all the data that i will be needing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data normalization\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "arraythatimimporting = []\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(arraythatimimporting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the network without being actually shaped to the imports we're doing \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import layers, models\n",
    "\n",
    "num_features = 0 #amount of features on import \n",
    "num_classes = 3 #amount of classes on output, doing classification\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape = (num_features,)),\n",
    "    layers.Dense(64, activation = 'relu'),\n",
    "    layers.Dense(32, activation = 'relu'), \n",
    "    layers.Dense(16, activation = 'relu'),\n",
    "    layers.Dense(num_classes, activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer = 'adam' , loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we need to fit and run the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
